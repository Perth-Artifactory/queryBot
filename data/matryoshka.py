import hashlib
import html
import json
import logging
import re
from typing import Optional

import openai
import requests

from data import reddit

with open("config.json","r") as f:
    config: dict = json.load(f)

def download_page(pagedata: dict) -> dict:
    """Accepts a dict containing a url and download_url and returns the same dict with the content of the page and the title of the page added"""
    p = requests.get(pagedata["download_url"])
    pagedata["content"] = html.unescape(p.text)
    titles = re.findall("<title[^>]*>(.*)<\/title>", p.text)
    if titles:
        pagedata["title"] = titles[0]
    else:
        pagedata["title"] = "Webpage"
    return pagedata

def mrkdwn(pagedata: dict) -> dict:
    """Accepts a dict containing a url, download_url and content and returns the same dict with the title of the page added if it's set via 'title: '"""
    pagedata["title"] = re.findall("\ntitle: '*?(.*)'*?", pagedata["content"])[0].replace("'","")
    return pagedata

def process_page(url: str) -> dict:
    """Accepts a url and returns a dict containing the url, download_url and a placeholder title. Download_url has been formatted by matching functions in url_conversions
    The download_url will be formatted by the first applicable entry in url_conversions. If no entry is applicable the url will be used as the download_url"""
    if url[0] == "<" and url[-1] == ">":
        url = url[1:-1]
    for block in url_conversions:
        if block["search"] in url:
            d_url = url
            for s in block["replace"]:
                d_url = d_url.replace(s[0],s[1])
            d_url += block["suffix"]
            i = {"url": url,
                 "download_url": d_url}
            for f in block["functions"]:
                i = f(i)
            return i
    i = {"url":url,"download_url":url,"title":"Webpage"}
    return download_page(i)

def process_pages(url: Optional[list] = None) -> dict:
    """Accepts a list of urls and returns a dict containing pagedata for each url using a hash of the url as the key. If no urls are provided it will use the urls in config["urls"]"""
    if url:
        urls = [url]
    else:
        urls = config["urls"]
    out = {}
    for url in urls:
        out[hashlib.md5(url.encode()).hexdigest()] = process_page(url)
    return out

def gpt_summarise(pagedata: dict) -> dict:
    """Accepts a dict containing a url, download_url, content and title and returns the same dict with a summary of the page added.
    When running live the summary will be generated by GPT-3.5-turbo. If the bot is running in development mode the summary will be a placeholder"""
    if config["bot"]["dev"]:
        pagedata["summary"] = "Page summary not grabbed because we're running in development mode"
        logging.info(f'Not summaring {pagedata["title"]} in development mode')
        return pagedata
    openai.api_key_path = './key'
    logging.info(f'initiating summary of {pagedata["title"]}')
    try:
        r = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": f'You are a helpful worker being run by {config["bot"]["org_name"]}. You are tasked with summarising the content of web pages in a format that can be used by other iterations of GPT.'},
                {"role": "user", "content": f'This is the contents of the webpage "{pagedata["title"]}"\n{pagedata["content"]}'}
                ]
            )
    except openai.error.InvalidRequestError:
        pagedata["summary"] = " "
        return pagedata
    pagedata["summary"] = r["choices"][0]["message"]["content"]
    return pagedata

def format_pages(pages: dict) -> list[dict]:
    """Accepts a dict containing pagedata for each url and returns a list of dicts containing prompts for each page. The prompts will be formatted for use by GPT-3.5-turbo"""
    prompts = []
    for k in pages:
        pagedata = pages[k]
        prompts.append({"role": "user", "content": f'There is a webpage titled {pagedata["title"]} at {pagedata["url"]} This is a summary of the page written by a version of GPT:\n{pagedata["summary"]}'})
    return prompts

def single_page(url: str) -> str:
    """Accepts a url and returns a string containing a prompt for GPT-3.5-turbo. This is primarily used by !url"""
    if url[0] == "<" and url[-1] == ">":
        url = url[1:-1]
    pages = process_pages(url=url)
    for page in pages:
        p = pages[page]
    if type(p) == str:
        return {"role": "user", "content": p}
    return f'There is a webpage titled {p["title"]} at {p["url"]} It contains:\n{p["content"]}'

url_conversions = [{"search":"wiki.artifactory.org.au/en/",
                    "replace":[("wiki.artifactory.org.au/en/","raw.githubusercontent.com/Perth-Artifactory/wiki/main/")],
                    "suffix":".md",
                    "functions":[download_page,mrkdwn]},
                  {"search":"artifactory.org.au/pages/",
                   "replace":[("artifactory.org.au/pages/","raw.githubusercontent.com/Perth-Artifactory/website/master/_pages/")],
                   "suffix":".md",
                   "functions":[download_page,mrkdwn]},
                  {"search":"reddit.com/r/",
                   "replace":[],
                   "suffix":"",
                   "functions":[reddit.format_post]},
                  {"search":"github.com",
                   "replace":[("github.com/","raw.githubusercontent.com/"),("/blob","")],
                   "suffix":"",
                   "functions":[download_page]},
                   ]

# Load cached website data/summaries
data_new = process_pages()
try:
    with open("babushka.json","r") as f:
        try:
            data_old: dict = json.load(f)
        except:
            data_old = {}
except FileNotFoundError:
    data_old = {}
data_final = {}
logging.info(f'Have {len(data_new)} items from config {len(data_old)} in cache')

# Check if we have a cached version of the page, if not summarise it
for d in data_new:
    if d not in data_old:
        logging.debug(f'{d} not in cache')
        data_final[d] = gpt_summarise(data_new[d])
    elif data_new[d]["content"] != data_old[d]["content"]:
        logging.debug(f'Version of {d} in cache is stale')
        data_final[d] = gpt_summarise(data_new[d])
    else:
        data_final[d] = data_old[d]

# Save the new cache
with open("babushka.json","w") as f:
    json.dump(data_final, f, indent=2)